{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Price Prediction\n",
    "\n",
    "## This project uses a recurrent neural network called Long Short Term Memory (LSTM) to predict cryptocurrency prices using previous closing prices and volume\n",
    "\n",
    "## Don't use this for real trading operations as this is for study purposes only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get public and private keys from my account on Binance to check cryptocurrency prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkey = \"   \" #Enter with public key\n",
    "Skey = \"   \" #Enter with private key\n",
    "\n",
    "client = Client(Pkey, Skey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating classify function to check if the coin should be sold (return 1) or not do anything (return 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating preprocess function to scale the data between 0 and 1 and to split the data into sequences of 60 values each (1 value per minute) so the neural network can learn how the prices behave in periods of 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.drop(\"Prediction\", 1)\n",
    "    \n",
    "    for col in df.columns:  # go through all of the columns\n",
    "        if col != 'Target':  # normalize all ... except for the target itself!\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
    "\n",
    "    df.dropna(inplace=True) #cleanup again\n",
    "    \n",
    "    \n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=seq_len)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == seq_len:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]]) #Append an numpy array with previous days and the label(target)\n",
    "    \n",
    "    random.shuffle(sequential_data)       \n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    for seq, Target in sequential_data:\n",
    "        if Target == 0:\n",
    "            sells.append([seq, Target])\n",
    "        elif Target == 1:\n",
    "            buys.append([seq, Target])\n",
    "    \n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    \n",
    "    lower = min(len(buys), len(sells)) #get which one is lower\n",
    "    \n",
    "    buys = buys[:lower] #buys doesn't go past the lower\n",
    "    sells = sells[:lower] #sells doesn't go past the lower\n",
    "    \n",
    "    sequential_data = buys+sells\n",
    "    \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, Target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(Target)\n",
    "    \n",
    "    return np.array(X), np.array(y) #make X and y numpy arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the close price and volume of every minute for the past month and add them to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 60 #defining the size of the sequences\n",
    "predict = 1 #predict one minute in the future\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "candles = client.get_historical_klines(\"BNBBTC\", Client.KLINE_INTERVAL_1MINUTE, \"1 month ago UTC\") # get prices per minute from last month till now\n",
    "candles_clean = pd.DataFrame(candles, columns=[\"Open_time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Close Time\", \"Quote_volume\", \"Num of Trades\", \"Taker Buy Base\", \"Taker Buy Quote\", \"Ignore\"]) #Create Pandas dataframe\n",
    "candles_last = candles_clean.drop(columns=[\"Open_time\", \"Open\", \"High\", \"Low\", \"Close Time\", \"Quote_volume\", \"Num of Trades\", \"Taker Buy Base\", \"Taker Buy Quote\", \"Ignore\"]) #Keeping just Close price and Volume\n",
    "candles_last[\"Prediction\"] = candles_last[[\"Close\"]].shift(-predict) # shift all the values 1 position to compare with future prices\n",
    "candles_last[\"Target\"] = list(map(classify, candles_last[\"Close\"], candles_last[\"Prediction\"])) #Creates target column, which is going to have values of 1 and 0(sell or don't do anything)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting 95% of the data into training data and 5% into validation data, and preprocess them both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 40184  Test data: 2130\n"
     ]
    }
   ],
   "source": [
    "times = sorted(candles_last.index.values)\n",
    "last_5pct = times[-int(0.05*len(times))]\n",
    "\n",
    "candles_validation = candles_last[(candles_last.index >= last_5pct)] # 5% of data for validation\n",
    "candles_train = candles_last[(candles_last.index < last_5pct)] # 95% of data for training\n",
    "\n",
    "validation_x, validation_y = preprocess_df(candles_validation) #preprocess validation data\n",
    "train_x, train_y = preprocess_df(candles_train) #preprocess training data\n",
    " \n",
    "print(\"Train data: \" + str(len(train_x)) + \"  \" + \"Test data: \" + str(len(validation_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40184 samples, validate on 2130 samples\n",
      "Epoch 1/10\n",
      "40184/40184 [==============================] - 19s 473us/sample - loss: 0.7444 - accuracy: 0.4956 - val_loss: 0.6952 - val_accuracy: 0.4972\n",
      "Epoch 2/10\n",
      "40184/40184 [==============================] - 14s 349us/sample - loss: 0.6988 - accuracy: 0.4987 - val_loss: 0.6941 - val_accuracy: 0.4981\n",
      "Epoch 3/10\n",
      "40184/40184 [==============================] - 14s 349us/sample - loss: 0.6952 - accuracy: 0.4998 - val_loss: 0.6948 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "40184/40184 [==============================] - 14s 350us/sample - loss: 0.6939 - accuracy: 0.4965 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "40184/40184 [==============================] - 14s 350us/sample - loss: 0.6936 - accuracy: 0.4998 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "40184/40184 [==============================] - 14s 352us/sample - loss: 0.6934 - accuracy: 0.5024 - val_loss: 0.6948 - val_accuracy: 0.4944\n",
      "Epoch 7/10\n",
      "40184/40184 [==============================] - 14s 352us/sample - loss: 0.6937 - accuracy: 0.5026 - val_loss: 0.6933 - val_accuracy: 0.4944\n",
      "Epoch 8/10\n",
      "40184/40184 [==============================] - 14s 351us/sample - loss: 0.6935 - accuracy: 0.4981 - val_loss: 0.6939 - val_accuracy: 0.4930\n",
      "Epoch 9/10\n",
      "40184/40184 [==============================] - 14s 352us/sample - loss: 0.6933 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.4906\n",
      "Epoch 10/10\n",
      "40184/40184 [==============================] - 14s 351us/sample - loss: 0.6933 - accuracy: 0.5007 - val_loss: 0.6933 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x292765ac188>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model (RNN - LSTM)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(validation_x, validation_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My overal accuracy was around 50% most of the time. I'm aware this is still not a great accuracy and the project can be improved much more. As I've mentioned before, this was made only for study purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Binance",
   "language": "python",
   "name": "binance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
